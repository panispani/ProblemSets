{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/panayiotis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/panayiotis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If not already downloaded, run this once to download wordnet and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_DATA_PATH = os.path.join('data', 'cases_metadata.csv')\n",
    "DATA_DIR = os.path.join('data', 'cases')\n",
    "NUM_CASES = 1000 # Perform analysis on 1000 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "cases = pd.read_csv(META_DATA_PATH)\n",
    "\n",
    "# Drop all cases with NaNs\n",
    "cases.dropna(subset=['x_republican'],inplace=True)\n",
    "cases.dropna(subset=['log_cites'],inplace=True)\n",
    "\n",
    "# Display how many NaNs there are per column\n",
    "# print(cases.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   caseid  case_reversed  judge_id    year  x_republican  log_cites  \\\n",
      "0  X3JGGO              0    1653.0  1925.0           1.0   1.098612   \n",
      "1  X3OH3J              0    1034.0  1924.0           0.0   1.609438   \n",
      "2  X3U0KO              0    2303.0  1925.0           0.0   1.791759   \n",
      "7  X3JGJV              0     485.0  1925.0           0.0   2.708050   \n",
      "8  X2S1PK              0    1113.0  1924.0           1.0   1.386294   \n",
      "\n",
      "                                                text  \n",
      "0   POLLOCK , District Judge.\\nFor convenience, t...  \n",
      "1   JOHNSON , Circuit Judge.\\nThis is a patent in...  \n",
      "2   WOOLLEY , Circuit Judge.\\nThe indictment agai...  \n",
      "7   DAVIS , Circuit Judge.\\nThe Beech-Nut Packing...  \n",
      "8   KENNEDY , District Judge.\\nThis is a suit in ...  \n"
     ]
    }
   ],
   "source": [
    "# Load texts and add to dataframe\n",
    "\n",
    "texts = []\n",
    "for caseid in cases.caseid:\n",
    "    file = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(caseid + '.txt')]\n",
    "    # Make sure you found the right file and is unique\n",
    "    assert len(file) == 1\n",
    "    text = open(file[0]).read()\n",
    "    texts.append(text)\n",
    "    \n",
    "cases['text'] = texts\n",
    "print(cases.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   caseid  case_reversed  judge_id    year  x_republican  log_cites  \\\n",
      "0  X3JGGO              0    1653.0  1925.0           1.0   1.098612   \n",
      "1  X3OH3J              0    1034.0  1924.0           0.0   1.609438   \n",
      "2  X3U0KO              0    2303.0  1925.0           0.0   1.791759   \n",
      "7  X3JGJV              0     485.0  1925.0           0.0   2.708050   \n",
      "8  X2S1PK              0    1113.0  1924.0           1.0   1.386294   \n",
      "\n",
      "                                                text  \\\n",
      "0   POLLOCK , District Judge.\\nFor convenience, t...   \n",
      "1   JOHNSON , Circuit Judge.\\nThis is a patent in...   \n",
      "2   WOOLLEY , Circuit Judge.\\nThe indictment agai...   \n",
      "7   DAVIS , Circuit Judge.\\nThe Beech-Nut Packing...   \n",
      "8   KENNEDY , District Judge.\\nThis is a suit in ...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  pollock district judge convenience party desig...  \n",
      "1  johnson circuit judge patent infringement suit...  \n",
      "2  woolley circuit judge indictment hudson brogan...  \n",
      "7  davis circuit judge beechnut packing company b...  \n",
      "8  kennedy district judge suit equity brought mis...  \n"
     ]
    }
   ],
   "source": [
    "# Data normalization and pre-processing\n",
    "\n",
    "def normalize(text):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    # Remove whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove weird non-printable characters\n",
    "    text = ''.join([c for c in text if c in string.printable])\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('','',punctuation) \n",
    "    text = text.translate(translator)\n",
    "    # Remove stopwords\n",
    "    stoplist = stopwords.words('english')\n",
    "    text = ' '.join(word for word in text.split() if word not in stoplist)\n",
    "    # lematize, need to pass words individually\n",
    "    text = ' '.join(nltk.WordNetLemmatizer().lemmatize(word) for word in text.split())\n",
    "    return text\n",
    "                   \n",
    "# Apply normalization to the text of each case\n",
    "cases['processed_text'] = cases['text'].apply(normalize)\n",
    "print(cases.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec small window=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller window we expect less..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec big window=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger window we expect.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
